{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import NLTK library and its packages\n",
    "\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert a Sentence into Tokens\n",
    "\n",
    "hint: use word_tokenize() function to convert a string to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', ',', 'this', 'is', 'a', 'nice', 'hotel', '.']\n"
     ]
    }
   ],
   "source": [
    "## Add in your code below\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(\"Hi, this is a nice hotel.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Words to their Base Forms\n",
    "\n",
    "There are two techniques that you can use to convert a word to its base. The first technique is stemming. Stemming is a simple algorithm that removes affixes from a word. There are various stemming algorithms available for use in NLTK. You will use the Porter algorithm in this tutorial.\n",
    "\n",
    "Let's convert the word 'going' to its base.\n",
    "\n",
    "Afer that, let's do it for 'constitutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n",
      "constitut\n"
     ]
    }
   ],
   "source": [
    "## Add in your code below\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem(\"going\"))\n",
    "\n",
    "print(stemmer.stem(\"constitutes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see an issue with the stemming result, this issue is solved by moving on to a more complex approach towards finding the base form of a word in a given context. The process is called lemmatization. Lemmatization normalizes a word based on the context and vocabulary of the text. In NLTK, you can lemmatize sentences using the WordNetLemmatizer class.\n",
    "\n",
    "First, you need to download the wordnet resource from the NLTK downloader in the Python terminal.\n",
    "\n",
    "Then, let's convert the word 'going' to its base.\n",
    "\n",
    "Afer that, let's do it for 'constitutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Brian.Ho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "\n",
    "#https://wordnet.princeton.edu/\n",
    "#WordNet® is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets \n",
    "#of cognitive synonyms (synsets), each expressing a distinct concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constitute\n"
     ]
    }
   ],
   "source": [
    "## Add in your code below\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lem = WordNetLemmatizer()\n",
    "print(lem.lemmatize('constitutes', 'v'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the context of every word in a sentence\n",
    "\n",
    "NLTK has a pos_tag function which helps in determining the context (grammar tag) of a word in a sentence. However, you first need to download the averaged_perceptron_tagger resource through the NLTK downloader. After that, use the word_tokenize function to tokenize the sentence and pos_tag to retrive the grammar tag. The sentence is 'Hi, this is a nice hotel.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Brian.Ho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi', 'NNP'), (',', ','), ('this', 'DT'), ('is', 'VBZ'), ('a', 'DT'), ('nice', 'JJ'), ('hotel', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "## Add in your code below\n",
    "from nltk.tag import pos_tag\n",
    "sample = \"Hi, this is a nice hotel.\"\n",
    "print(pos_tag(word_tokenize(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you decode the context of each token? Here is a full list of all tags and their corresponding meanings on the web. Notice that the tags of all nouns begin with “N”, and for all verbs begin with “V”. \n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert every Word to their Base Forms according to its context\n",
    "\n",
    "Here you'd need to define a function that takes in the tokenized words in a list. To tokenize the given sentence, you'd need to use the word_tokenize method.\n",
    "\n",
    "In the function, first create an instance of the WordNetLemmatizer class. Then, create an empty list with name 'lemmatized_tokens' to store the base form words to be returned to main code. \n",
    "\n",
    "For every token, you'd need to find it's grammar tag using the pos_tag function. In order to effectively find the based form of a word using the lemmatize function of the WordNetLemmatizer class, the lemmatize function needs two inputs - token and its correct grammar tag in single letter. What you need to do is to find the longer version grammar tag of every token, if its first two letters is 'NN' then it's a noun, if it's 'VB' then it's a verb, else it's a adjective. \n",
    "\n",
    "The sentence is 'Legal authority constitutes all magistrates.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Legal NNP\n",
      "authority NN\n",
      "constitutes VBZ\n",
      "all DT\n",
      "magistrates NNS\n",
      ". .\n",
      "['Legal', 'authority', 'constitute', 'all', 'magistrate', '.']\n"
     ]
    }
   ],
   "source": [
    "## Add in your code below\n",
    "\n",
    "def lemmatize_tokens(tokens_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in pos_tag(tokens_list):\n",
    "        print(word, tag)\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_tokens\n",
    "\n",
    "sample = \"Legal authority constitutes all magistrates.\"\n",
    "print(lemmatize_tokens(word_tokenize(sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "The next step in preparing data is to clean the data and remove anything that does not add meaning to your analysis. Broadly, we will look at removing punctuation and stop words from your analysis.\n",
    "\n",
    "Removing punctuation is a fairly easy task. The punctuation object of the string library contains all the punctuation marks in English.\n",
    "\n",
    "Remove the punctuations from the sentence 'PETALING JAYA: Malaysians can expect the haze situation to improve once the monsoon transition phase kicks in next week.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PETALING', 'JAYA', 'Malaysians', 'can', 'expect', 'the', 'haze', 'situation', 'to', 'improve', 'once', 'the', 'monsoon', 'transition', 'phase', 'kicks', 'in', 'next', 'week']\n"
     ]
    }
   ],
   "source": [
    "## Add in your code below\n",
    "nopunct_tokens = []\n",
    "sent = 'PETALING JAYA: Malaysians can expect the haze situation to improve once the monsoon transition phase kicks in next week.'\n",
    "for token in word_tokenize(sent):\n",
    "    if token not in string.punctuation:\n",
    "        nopunct_tokens.append(token)\n",
    "        \n",
    "print(nopunct_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stop Words\n",
    "\n",
    "Next, we will focus on removing stop words. Stop words are commonly used words in language like “I”, “a” and “the”, which add little meaning to text when analyzing it. We will therefore, remove stop words from our analysis. First, download the stopwords resource from the NLTK downloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Brian.Ho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your download is complete, import stopwords from nltk.corpus and use the .words() method with “english” as the argument. It is a list of 179 stop words in the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add in your code below\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the lemmatization example with the concepts discussed in this section to create the following function, clean_data(). Additionally, before comparing if a word is a part of the stop words list, we convert it to the lower case. This way, we still capture a stop word if it occurs at the start of a sentence and is capitalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Legal', 'authority', 'constitute', 'magistrate']\n"
     ]
    }
   ],
   "source": [
    "## Add in your code below\n",
    "\n",
    "def clean_data(tokens_list, stop_words):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cleaned_tokens  = []\n",
    "    for word, tag in pos_tag(tokens_list):\n",
    "#         print(word, tag)\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        \n",
    "        word = lemmatizer.lemmatize(word, pos)\n",
    "        \n",
    "        ## Add in your code below\n",
    "        if word not in string.punctuation and word.lower() not in stop_words:\n",
    "            cleaned_tokens.append(word)\n",
    "        ## Finish your code here\n",
    "        \n",
    "    return cleaned_tokens\n",
    "\n",
    "sample = \"Legal authority constitutes all magistrates.\"\n",
    "print(clean_data(word_tokenize(sample), stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Distribution\n",
    "\n",
    "Now that you are familiar with the basic cleaning techniques in NLP, let’s try and find the frequency of words in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = 'PETALING JAYA: Malaysians can expect the haze situation to improve once the monsoon transition phase kicks in next week, as thunderstorms and heavy downpours are expected to take place across the country. The Malaysian Meteorological Department said the monsoon transition phase is expected to begin next Tuesday (Sept 24) and will last until the beginning of November. This marks the end of the Southwest Monsoon period, which began on May 6, said the department. During this phase, regional areas of the country will receive wind from various directions with low speeds, which poses as a potential for the formation of a thunderstorm, it said in a statement on Saturday (Sept 21). Thunderstorms along with heavy downpours and strong winds are expected during the evening and night in the West Coast and interior areas of Peninsular Malaysia, the west coast of Sabah, and central Sarawak, as well as its west coast. While the rainy weather has the potential to cause flash floods and damage towards weak infrastructures, the department said the haze situation is expected to improve. The public is advised to be more alert during this period and always stay updated with weather predictions and warning by the department through its website, myCuaca app and social media.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the frequency distribution of words in your text, you can use FreqDist class of NLTK. Initialize the class with the tokens as an argument. Then use the .most_common() method to find the commonly occurring terms. Let us try and find the top ten terms in this case.\n",
    "\n",
    "- First you'd need to token the the 'news' paragraph\n",
    "- Call the clean_data function using the token list from above\n",
    "- Use the FreqDist class with the cleaned tokens as an argument\n",
    "- Use the most_common() method find the top 10 most common occurring terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('expect', 5),\n",
       " ('say', 4),\n",
       " ('phase', 3),\n",
       " ('department', 3),\n",
       " ('haze', 2),\n",
       " ('situation', 2),\n",
       " ('improve', 2),\n",
       " ('monsoon', 2),\n",
       " ('transition', 2),\n",
       " ('next', 2)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Add in your code below\n",
    "from nltk import FreqDist\n",
    "\n",
    "tokens = word_tokenize(news)\n",
    "cleaned_tokens = clean_data(tokens, stop_words)\n",
    "freq_dist = FreqDist(cleaned_tokens)\n",
    "freq_dist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
